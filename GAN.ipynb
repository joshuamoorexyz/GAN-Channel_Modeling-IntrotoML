{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e8e39d-2264-4f94-a3e7-c36b074880ae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><h1> CSE 4693/6693 Intro to Machine Learning </h1></center>\n",
    "\n",
    "## Project name: GAN Channel Modeling (Group 4)\n",
    "\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "\n",
    "| Name | NetID |\n",
    "|:-----|:------|\n",
    "| Joshua Moore | jjm702 |\n",
    "| Tirian Judy | tkj105 |\n",
    "| Claire Johnson | kj1289 |\n",
    "| Aayam Raj Shakya | as5160 |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073aedf-e0d0-4070-8bf2-346796f94d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports & some predefines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Use cuda if possible; otherwise eat some threads on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read one of the data files #TODO: Read all csvs\n",
    "\n",
    "features = ['center_frequency', 'distance', 'h_dist', 'v_dist', 'avgPower', 'avgSnr',\n",
    "            'freq_offset', 'avg_pl', 'aod_theata', 'aoa_theta', 'aoa_phi',\n",
    "            'pitch', 'yaw', 'roll', 'vel_x', 'vel_y', 'vel_z', 'speed', 'avg_pl_rolling', 'avg_pl_ewma']\n",
    "\n",
    "df = pd.read_csv(\"/content/2023-12-15_15_41-results.csv\", usecols = features)\n",
    "\n",
    "\n",
    "#Need to ensure only valid data goes beyond here #TODO\n",
    "\n",
    "data = df[features].values#[:100] Limit values be needed - ask\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "# Convert data to PyTorch tensors - no filter currently, reproduce everything\n",
    "trainX = torch.tensor(X, dtype=torch.float32)\n",
    "trainY = torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy of Aayam's LSMT class - might need to be altered?\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        # If hidden and cell states are not provided, initialize them as zeros\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Forward pass through LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Selecting the last output\n",
    "        return out, hn, cn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = LSTMModel(input_dim=len(features), hidden_dim=50, layer_dim=1, output_dim=len(features))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop - LSTM\n",
    "num_epochs = 4000\n",
    "h0, c0 = None, None  # Initialize hidden and cell states\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, h0, c0 = model(trainX, h0, c0)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Detach hidden and cell states to prevent backpropagation through the entire sequence\n",
    "    h0 = h0.detach()\n",
    "    c0 = c0.detach()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - validate outputs now that it should be for multiple values not just SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Generator\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, inputSize=6, outputSize =(48,48)): #LSTM model should produce a vector of 6 terms; Output an array of 48H * 48W\n",
    "        super(Generator, self).__init__() #We should be fine using Pytorch base model code\n",
    "\n",
    "        self.outputSize = outputSize #Need to remember the output size for the forward function\n",
    "\n",
    "        self.model = nn.Sequential(nn.Linear(inputSize, 128), #mapping input vector to 128 neurons; \n",
    "                                   nn.LeakyReLU(0.2, inplace=True), #Using inplace=True to save memory + it seems to be standard practice\n",
    "                                   #start hidden layers\n",
    "                                   nn.Linear(128,256), #Hidden Layer 1; Upscaling by factor of 2\n",
    "                                   nn.BatchNorm1d(256), #Normalization\n",
    "                                   nn.LeakyReLU(0.2, inplace=True), \n",
    "                                   nn.Linear(256, 512), #Hidden Layer 2; Upsacling by factor of 2; Last hidden layer for now\n",
    "                                   nn.BatchNorm1d(512), #Normalization\n",
    "                                   nn.LeakyReLu(0.2, inplace=True),\n",
    "                                   nn.Linear(512, outputSize[0] * outputSize[1]), #Output layer\n",
    "                                   nn.Tanh() #Normalization for output (-1, 1)\n",
    "                                   )\n",
    "    \n",
    "    #Passes data into first layer & executes sequentially based upon params from self.model()\n",
    "    def forward(self, noise):\n",
    "        genImage =  self.model(noise) #Grab output tensor\n",
    "        return genImage.view(-1, 1, self.outputSize[0], self.outputSize[1]) #Should produce a grey scale image using the view function. Args: Batchsize(-1 means figure it out for me), numChannels, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f12be-c549-4a54-9549-59a1cced6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, inputSize=(48,48)): #Simple classifier returns [0,1]; 1 real\n",
    "        super(Discriminator, self).__init__() #Use the base Pytorch discriminator\n",
    "\n",
    "        self.inputSize = inputSize #Need to remember the input size for the forward function\n",
    "\n",
    "        self.model = nn.Sequential(nn.Linear(inputSize[0] * inputSize[1], 512), #mapping input vector to 512 neurons\n",
    "                                   nn.LeakyReLU(0.2, inplace=True),\n",
    "                                   nn.Linear(512, 256), #Hidden Layer 1; Downscaling by factor of 2\n",
    "                                   nn.LeakyReLU(0.2, inplace=True),\n",
    "                                   nn.Linear(256, 1), #Output layer - Map 256 neurons into the probability of being real\n",
    "                                   nn.Sigmoid() #Turn the output into [0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        flatInput = input.view(input.size(0), -1) #Flatten the input to 1D\n",
    "        return self.model(flatInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be700a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data import DataLoader\n",
    "\n",
    "#Hyper params\n",
    "GANepochs = 1000\n",
    "GANlr = 0.0002 #Should probably be low\n",
    "\n",
    "#Labels - DO NOT USE 1 or 0, it will be to ridged, smooth it by ~.025 - .2\n",
    "realLabel = 0.9\n",
    "fakeLabel = 0.1\n",
    "\n",
    "#Loss function\n",
    "losFunc = nn.BCELoss()\n",
    "\n",
    "#Creating the Discriminator\n",
    "discrim = Discriminator().to(DEVICE)\n",
    "dOptimizer = torch.optim.Adam(discrim.parameters(), lr=0.0002, betas=(0.5, 0.999)) #Lower B1 since models will fight eachother\n",
    "\n",
    "#Creating the Generator\n",
    "generator = Generator().to(DEVICE)\n",
    "gOptimizer = torch.optim.Adam(generator.parameter(), lr=0.0002, betas=(0.5, 0.999)) #Lower B1 since models will fight eachother\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#load data somehow\n",
    "#It would be visualizations of the data no?\n",
    "dataloaded = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=None, num_workers=0, pin_memory=False, tiemout=0)\n",
    "\n",
    "#static noise\n",
    "fixedNoise = torch.randn(10, len(features)).to(DEVICE)\n",
    "\n",
    "for epoch in range(GANepochs):\n",
    "    #Discriminator goes to the gym\n",
    "    for i, realSpectrogram in enumerate(dataloaded):\n",
    "        realSpectrogram = realSpectrogram.to(DEVICE) #Shove it on the GPU\n",
    "        \n",
    "        real = torch.full((len(realSpectrogram), 1), realLabel, device=DEVICE)\n",
    "        fake = torch.full((len(realSpectrogram), 1), fakeLabel, device=DEVICE)\n",
    "\n",
    "        #Discrim on the real images\n",
    "        dOptimizer.zero_grad() #0 gradient\n",
    "        output = discrim()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
