{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c073aedf-e0d0-4070-8bf2-346796f94d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports & some predefines\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Use cuda if possible; otherwise eat some threads on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fff9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('Figures/SNR', transform=transform)\n",
    "dataloaded = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e718cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read one of the data files #TODO: Read all csvs\n",
    "\n",
    "features = ['center_freq', 'dist', 'h_dist', 'v_dist', 'avgPower', 'avgSnr',\n",
    "            'freq_offset', 'avg_pl', 'aod_theta', 'aoa_theta', 'aoa_phi',\n",
    "            'pitch', 'yaw', 'roll', 'vel_x', 'vel_y', 'vel_z', 'speed', 'avg_pl_rolling', 'avg_pl_ewma']\n",
    "\n",
    "df = pd.read_csv(\"dataset/2023-12-15_15_41-results.csv\", usecols = features)\n",
    "\n",
    "\n",
    "#Need to ensure only valid data goes beyond here #TODO\n",
    "\n",
    "data = df[features].values#[:100] Limit values be needed - ask\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "# Convert data to PyTorch tensors - no filter currently, reproduce everything\n",
    "trainX = torch.tensor(X, dtype=torch.float32)\n",
    "trainY = torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2d0e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Copy of Aayam's LSMT class - might need to be altered?\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.layer_dim = layer_dim\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x, h0=None, c0=None):\n",
    "#         # If hidden and cell states are not provided, initialize them as zeros\n",
    "#         if h0 is None or c0 is None:\n",
    "#             h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#             c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "#         # Forward pass through LSTM\n",
    "#         out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])  # Selecting the last output\n",
    "#         return out, hn, cn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ed9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize model, loss, and optimizer\n",
    "# model = LSTMModel(input_dim=len(features), hidden_dim=50, layer_dim=1, output_dim=len(features))\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "542241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training loop - LSTM\n",
    "# num_epochs = 4000\n",
    "# h0, c0 = None, None  # Initialize hidden and cell states\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # Forward pass\n",
    "#     outputs, h0, c0 = model(trainX, h0, c0)\n",
    "\n",
    "#     # Compute loss\n",
    "#     loss = criterion(outputs, trainY)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Detach hidden and cell states to prevent backpropagation through the entire sequence\n",
    "#     h0 = h0.detach()\n",
    "#     c0 = c0.detach()\n",
    "\n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ec4f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - validate outputs now that it should be for multiple values not just SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d5c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Generator\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, inputSize=6, outputSize =(256,256)): #Default input size is 6 (likely needs to be changed); output size is 800x800\n",
    "        super(Generator, self).__init__() #We should be fine using Pytorch base model code\n",
    "\n",
    "        self.outputSize = outputSize #Need to remember the output size for the forward function\n",
    "\n",
    "        self.model = nn.Sequential(nn.Linear(inputSize, 128), #mapping input vector to 128 neurons; \n",
    "                                   nn.LeakyReLU(0.2, inplace=True), #Using inplace=True to save memory + it seems to be standard practice\n",
    "                                   #start hidden layers\n",
    "                                   nn.Linear(128,256), #Hidden Layer 1; Upscaling by factor of 2\n",
    "                                   nn.BatchNorm1d(256), #Normalization\n",
    "                                   nn.LeakyReLU(0.2, inplace=True), \n",
    "                                   nn.Linear(256, 512), #Hidden Layer 2; Upsacling by factor of 2; Last hidden layer for now\n",
    "                                   nn.BatchNorm1d(512), #Normalization\n",
    "                                   nn.LeakyReLU(0.2, inplace=True),\n",
    "                                   nn.Linear(512, outputSize[0] * outputSize[1] * 3), #Output layer; *3 for RGB \n",
    "                                   nn.Tanh() #Normalization for output [-1, 1]\n",
    "                                   )\n",
    "    \n",
    "    #Passes data into first layer & executes sequentially based upon params from self.model()\n",
    "    def forward(self, noise):\n",
    "        genImage =  self.model(noise) #Grab output tensor\n",
    "        return genImage.view(-1, 3, self.outputSize[0], self.outputSize[1]) #Should produce a RGB image using the view function. Args: Batchsize(-1 means figure it out for me), numChannels, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f12be-c549-4a54-9549-59a1cced6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, inputSize=(256,256)): #Simple classifier returns [0,1]; 1 real\n",
    "        super(Discriminator, self).__init__() #Use the base Pytorch discriminator\n",
    "\n",
    "        self.inputSize = inputSize #Need to remember the input size for the forward function\n",
    "\n",
    "        #in channels 3 = RGB data; stride 2 = downscale (1 for no downscale); padding 1 = keep the same size\n",
    "        self.model = nn.Sequential(nn.Conv2d(in_channels =3, out_channels=64, kernel_size=4, stride=2, padding=1), #Convolutional layer 1\n",
    "                                   nn.LeakyReLU(0.2, inplace=True), #Activation function\n",
    "\n",
    "                                   nn.Conv2d(64, 128, 4, 2, 1), #Convolutional layer 2 [batchSize, 64, 128, 128]\n",
    "                                   nn.BatchNorm2d(128), #Normalization\n",
    "                                   nn.LeakyReLU(0.2, inplace=True), #Activation function\n",
    "\n",
    "                                   nn.Conv2d(128, 256, 4, 2, 1), #Convolutional layer 3 [batchSize, 128, 64, 64]\n",
    "                                   nn.BatchNorm2d(256), #Normalization\n",
    "                                   nn.LeakyReLU(0.2, inplace=True), #Activation function\n",
    "                                   \n",
    "                                   nn.Conv2d(256, 512, 4, 2, 1), #Convolutional layer 4 [batchSize, 256, 32, 32]\n",
    "                                   nn.BatchNorm2d(512), #Normalization\n",
    "                                   nn.LeakyReLU(0.2, inplace=True), #Activation function\n",
    "\n",
    "                                   nn.Conv2d(512, 1, 4, 1, 0), #Output layer [batchSize, 512, 16, 16]\n",
    "\n",
    "                                   #Hypothetically we could flatten the output and use a linear layer to get the sigmoid\n",
    "                                #    nn.Flatten(), #Flatten the output\n",
    "                                #    nn.Linear(512 * 49 * 49, 1), #Linear layer to get the sigmoid\n",
    "\n",
    "                                   nn.Sigmoid() #Turn the output into [0,1] [batchSize, 1, 13, 13]\n",
    "        )\n",
    "    \n",
    "    #Passes data into first layer & executes sequentially based upon params from self.model() Implicitly called when given input.\n",
    "    def forward(self, input):\n",
    "        # Ensure the input has a channel dimension (batch_size, 1, 800, 800)\n",
    "        if len(input.shape) == 3:  # JIC somehow we got greyscale images\n",
    "            input = input.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        output = self.model(input) #[9,1,13,13]\n",
    "        print(f\"0: {output[0]}\")\n",
    "        print(f\"1: {output[1]}\")\n",
    "        print(f\"2: {output[2]}\")\n",
    "        print(f\"3: {output[3]}\")\n",
    "        output = output.view(-1, 1) #Flatten the output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60da67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4be700a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: tensor([[[0.4728, 0.5326, 0.4530, 0.5169, 0.5079, 0.5119, 0.4801, 0.4431,\n",
      "          0.4309, 0.5242, 0.4076, 0.3619, 0.3728],\n",
      "         [0.3898, 0.4609, 0.4798, 0.5161, 0.5370, 0.5151, 0.4677, 0.5196,\n",
      "          0.4251, 0.6026, 0.3325, 0.3651, 0.4706],\n",
      "         [0.5397, 0.5817, 0.5389, 0.5643, 0.5393, 0.5273, 0.5381, 0.5629,\n",
      "          0.4761, 0.5206, 0.3132, 0.4870, 0.4953],\n",
      "         [0.4624, 0.6827, 0.5225, 0.5316, 0.5315, 0.5315, 0.5328, 0.5339,\n",
      "          0.5147, 0.4974, 0.3262, 0.3896, 0.4413],\n",
      "         [0.5226, 0.5432, 0.5268, 0.5350, 0.5346, 0.5354, 0.5337, 0.5356,\n",
      "          0.5185, 0.5494, 0.4618, 0.4518, 0.4339],\n",
      "         [0.5442, 0.5933, 0.5828, 0.5473, 0.5473, 0.5456, 0.5391, 0.5365,\n",
      "          0.4648, 0.5537, 0.3894, 0.3726, 0.3424],\n",
      "         [0.5261, 0.5692, 0.6212, 0.5933, 0.5933, 0.5926, 0.5931, 0.5883,\n",
      "          0.5243, 0.6258, 0.4491, 0.3654, 0.4214],\n",
      "         [0.5464, 0.5864, 0.6380, 0.5863, 0.5863, 0.5864, 0.5910, 0.5877,\n",
      "          0.5069, 0.5996, 0.4125, 0.3704, 0.4859],\n",
      "         [0.5566, 0.5616, 0.6402, 0.6660, 0.6660, 0.6655, 0.6663, 0.6653,\n",
      "          0.5937, 0.6076, 0.4893, 0.3730, 0.5354],\n",
      "         [0.4367, 0.5816, 0.5371, 0.5443, 0.5443, 0.5434, 0.5441, 0.5440,\n",
      "          0.5022, 0.5500, 0.3539, 0.3801, 0.5125],\n",
      "         [0.4742, 0.5151, 0.4776, 0.4891, 0.4910, 0.4864, 0.4734, 0.4882,\n",
      "          0.4464, 0.5043, 0.3879, 0.4049, 0.4173],\n",
      "         [0.5933, 0.5877, 0.5811, 0.6050, 0.6313, 0.5910, 0.6252, 0.6231,\n",
      "          0.6523, 0.6509, 0.5135, 0.4804, 0.5523],\n",
      "         [0.4795, 0.5039, 0.5481, 0.5389, 0.5799, 0.5522, 0.5500, 0.5522,\n",
      "          0.5359, 0.5252, 0.4534, 0.4572, 0.5775]]], grad_fn=<SelectBackward0>)\n",
      "1: tensor([[[0.4636, 0.4796, 0.4054, 0.4611, 0.4350, 0.4627, 0.4333, 0.4259,\n",
      "          0.4073, 0.4852, 0.3838, 0.3311, 0.3753],\n",
      "         [0.3909, 0.3613, 0.4038, 0.4924, 0.5227, 0.4943, 0.4756, 0.4932,\n",
      "          0.4211, 0.5363, 0.3587, 0.4161, 0.4602],\n",
      "         [0.4137, 0.3907, 0.4488, 0.4101, 0.4169, 0.3934, 0.4209, 0.4183,\n",
      "          0.3860, 0.4826, 0.3462, 0.5496, 0.4392],\n",
      "         [0.4468, 0.4808, 0.4857, 0.5272, 0.5272, 0.5273, 0.5274, 0.5219,\n",
      "          0.5260, 0.4429, 0.4028, 0.4795, 0.4150],\n",
      "         [0.5170, 0.6251, 0.5747, 0.6096, 0.6114, 0.6114, 0.6098, 0.6129,\n",
      "          0.6176, 0.6211, 0.5069, 0.4859, 0.3740],\n",
      "         [0.4362, 0.5964, 0.6206, 0.6285, 0.6288, 0.6287, 0.6271, 0.6237,\n",
      "          0.6150, 0.5323, 0.4758, 0.4480, 0.3313],\n",
      "         [0.5123, 0.4861, 0.4786, 0.4745, 0.4739, 0.4731, 0.4714, 0.4668,\n",
      "          0.4651, 0.4903, 0.4089, 0.4090, 0.3625],\n",
      "         [0.3453, 0.3863, 0.3745, 0.3822, 0.3827, 0.3842, 0.3804, 0.3784,\n",
      "          0.4111, 0.4033, 0.4213, 0.4485, 0.4722],\n",
      "         [0.4607, 0.5466, 0.5175, 0.4762, 0.4755, 0.4751, 0.4755, 0.4773,\n",
      "          0.4899, 0.4576, 0.4359, 0.3954, 0.4258],\n",
      "         [0.3416, 0.4097, 0.4242, 0.4708, 0.4716, 0.4687, 0.4714, 0.4709,\n",
      "          0.4943, 0.5427, 0.3784, 0.4245, 0.4732],\n",
      "         [0.5733, 0.5698, 0.5673, 0.5989, 0.5809, 0.5861, 0.5810, 0.5958,\n",
      "          0.5505, 0.5435, 0.4070, 0.4496, 0.4205],\n",
      "         [0.3112, 0.4972, 0.5031, 0.4154, 0.4533, 0.4554, 0.4375, 0.4061,\n",
      "          0.4572, 0.6059, 0.5381, 0.5406, 0.5445],\n",
      "         [0.4354, 0.6228, 0.4465, 0.5160, 0.5958, 0.4985, 0.5482, 0.5802,\n",
      "          0.5465, 0.6338, 0.4899, 0.5002, 0.5373]]], grad_fn=<SelectBackward0>)\n",
      "2: tensor([[[0.4006, 0.5566, 0.5103, 0.5246, 0.5176, 0.5275, 0.4760, 0.4550,\n",
      "          0.4705, 0.5822, 0.3778, 0.3202, 0.4094],\n",
      "         [0.3646, 0.3925, 0.4231, 0.4628, 0.4701, 0.4650, 0.4064, 0.4804,\n",
      "          0.4002, 0.5954, 0.2964, 0.3881, 0.4563],\n",
      "         [0.4723, 0.5612, 0.5047, 0.5225, 0.5171, 0.4908, 0.5207, 0.5351,\n",
      "          0.4403, 0.5524, 0.3770, 0.5598, 0.5082],\n",
      "         [0.5791, 0.6365, 0.6108, 0.6172, 0.6185, 0.6185, 0.6185, 0.6201,\n",
      "          0.5815, 0.6237, 0.3990, 0.4551, 0.4270],\n",
      "         [0.5246, 0.6249, 0.6072, 0.6114, 0.6118, 0.6118, 0.6118, 0.6132,\n",
      "          0.5802, 0.6371, 0.4968, 0.5088, 0.3855],\n",
      "         [0.6046, 0.7235, 0.6643, 0.6615, 0.6617, 0.6617, 0.6617, 0.6612,\n",
      "          0.5841, 0.6362, 0.4446, 0.4323, 0.3463],\n",
      "         [0.5059, 0.5995, 0.5871, 0.5603, 0.5603, 0.5603, 0.5603, 0.5648,\n",
      "          0.4935, 0.5191, 0.4119, 0.3572, 0.4105],\n",
      "         [0.5737, 0.5037, 0.5314, 0.5473, 0.5473, 0.5473, 0.5473, 0.5486,\n",
      "          0.4776, 0.5367, 0.4210, 0.3624, 0.4687],\n",
      "         [0.3855, 0.5731, 0.5809, 0.5299, 0.5299, 0.5299, 0.5299, 0.5304,\n",
      "          0.4968, 0.5184, 0.3924, 0.3985, 0.5153],\n",
      "         [0.4263, 0.5234, 0.4550, 0.4708, 0.4709, 0.4708, 0.4697, 0.4784,\n",
      "          0.4635, 0.5492, 0.3524, 0.3936, 0.5144],\n",
      "         [0.5346, 0.6738, 0.6056, 0.5922, 0.5719, 0.5868, 0.5866, 0.6088,\n",
      "          0.5224, 0.6460, 0.4691, 0.4568, 0.4323],\n",
      "         [0.5392, 0.4850, 0.5710, 0.5976, 0.6193, 0.6377, 0.5881, 0.6345,\n",
      "          0.5733, 0.6932, 0.5884, 0.5150, 0.5997],\n",
      "         [0.4927, 0.4680, 0.5158, 0.5357, 0.5386, 0.4231, 0.5283, 0.5410,\n",
      "          0.4604, 0.5888, 0.3732, 0.4533, 0.4985]]], grad_fn=<SelectBackward0>)\n",
      "3: tensor([[[0.4620, 0.5612, 0.4733, 0.5233, 0.4931, 0.5287, 0.4835, 0.4627,\n",
      "          0.4770, 0.5671, 0.3760, 0.3366, 0.3815],\n",
      "         [0.4469, 0.5672, 0.5620, 0.5969, 0.6085, 0.5964, 0.5469, 0.6049,\n",
      "          0.5054, 0.6117, 0.3309, 0.3904, 0.4593],\n",
      "         [0.5345, 0.5552, 0.5465, 0.5397, 0.5386, 0.5218, 0.5396, 0.5381,\n",
      "          0.4867, 0.6115, 0.3269, 0.4872, 0.4796],\n",
      "         [0.5454, 0.6781, 0.6539, 0.6507, 0.6528, 0.6531, 0.6531, 0.6500,\n",
      "          0.6044, 0.5437, 0.4603, 0.4498, 0.3683],\n",
      "         [0.4269, 0.5420, 0.5749, 0.5475, 0.5469, 0.5469, 0.5469, 0.5470,\n",
      "          0.5231, 0.5706, 0.4568, 0.4089, 0.3757],\n",
      "         [0.5625, 0.6247, 0.6357, 0.5996, 0.5998, 0.5998, 0.5998, 0.5961,\n",
      "          0.5652, 0.5719, 0.4940, 0.3863, 0.2936],\n",
      "         [0.3755, 0.4572, 0.3587, 0.3854, 0.3855, 0.3855, 0.3855, 0.3855,\n",
      "          0.4188, 0.4934, 0.3791, 0.3780, 0.3967],\n",
      "         [0.6840, 0.6976, 0.6904, 0.7119, 0.7109, 0.7121, 0.7129, 0.7084,\n",
      "          0.6778, 0.6007, 0.5559, 0.4017, 0.4791],\n",
      "         [0.5310, 0.5356, 0.5211, 0.5181, 0.5246, 0.5211, 0.5228, 0.5216,\n",
      "          0.4986, 0.5783, 0.3914, 0.3972, 0.4981],\n",
      "         [0.5359, 0.6342, 0.5990, 0.5416, 0.5368, 0.5387, 0.5342, 0.5322,\n",
      "          0.5606, 0.5496, 0.3745, 0.4035, 0.4810],\n",
      "         [0.5058, 0.6272, 0.5955, 0.5725, 0.5702, 0.5826, 0.5725, 0.5569,\n",
      "          0.5461, 0.5418, 0.4336, 0.4356, 0.4213],\n",
      "         [0.4691, 0.3980, 0.4328, 0.3709, 0.4365, 0.4192, 0.4138, 0.4177,\n",
      "          0.4146, 0.5241, 0.4474, 0.4094, 0.4836],\n",
      "         [0.6095, 0.5709, 0.5527, 0.5307, 0.5776, 0.6162, 0.6401, 0.5797,\n",
      "          0.5711, 0.6214, 0.4952, 0.5604, 0.5308]]], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([1521, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m dOptimizer.zero_grad() \u001b[38;5;66;03m#Zero gradients before training begins\u001b[39;00m\n\u001b[32m     52\u001b[39m output = discrim(realSpectrogram)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m dLossOnReal = \u001b[43mlossFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m#Discrim on fake data\u001b[39;00m\n\u001b[32m     56\u001b[39m noise = torch.randn(realSpectrogram.size(\u001b[32m0\u001b[39m), \u001b[38;5;28mlen\u001b[39m(features)).to(DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiria\\OneDrive\\Desktop\\IML-WirelessGAN\\GAN-Channel_Modeling-IntrotoML\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiria\\OneDrive\\Desktop\\IML-WirelessGAN\\GAN-Channel_Modeling-IntrotoML\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiria\\OneDrive\\Desktop\\IML-WirelessGAN\\GAN-Channel_Modeling-IntrotoML\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[39m, in \u001b[36mBCELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tiria\\OneDrive\\Desktop\\IML-WirelessGAN\\GAN-Channel_Modeling-IntrotoML\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3560\u001b[39m, in \u001b[36mbinary_cross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3558\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target.size() != \u001b[38;5;28minput\u001b[39m.size():\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3561\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is deprecated. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure they have the same size.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3563\u001b[39m     )\n\u001b[32m   3565\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3566\u001b[39m     new_size = _infer_size(target.size(), weight.size())\n",
      "\u001b[31mValueError\u001b[39m: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([1521, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('Figures/SNR', transform=transform)\n",
    "dataloaded = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#Hyper params\n",
    "GANepochs = 5\n",
    "GANlr = 0.0002 #Should probably be low\n",
    "\n",
    "#Labels - DO NOT USE 1 or 0, it will be to ridged, smooth it by ~.025 - .2\n",
    "realLabel = 0.9\n",
    "fakeLabel = 0.1\n",
    "\n",
    "#Loss function\n",
    "lossFunc = nn.BCELoss()\n",
    "\n",
    "#Creating the Discriminator\n",
    "discrim = Discriminator().to(DEVICE)\n",
    "dOptimizer = torch.optim.Adam(discrim.parameters(), lr=0.0002, betas=(0.5, 0.999)) #Lower B1 since models will fight eachother\n",
    "\n",
    "#Creating the Generator\n",
    "generator = Generator().to(DEVICE)\n",
    "gOptimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999)) #Lower B1 since models will fight eachother\n",
    "\n",
    "\n",
    "#load data somehow\n",
    "#It would be visualizations of the data no?\n",
    "\n",
    "#static noise\n",
    "#fixedNoise = torch.randn(10, len(features)).to(DEVICE)\n",
    "\n",
    "for epoch in range(GANepochs):\n",
    "    \n",
    "    for i, (realSpectrogram, _) in enumerate(dataloaded):\n",
    "        #realSpectrogram.shape = torch.Size([9, 3, 256, 256])\n",
    "        \n",
    "        #Discriminator Training\n",
    "        realSpectrogram = realSpectrogram.to(DEVICE) #Shove it on the GPU\n",
    "        \n",
    "        real = torch.full((realSpectrogram.size(0), 1), realLabel, device=DEVICE)  # Ensure it has shape [batch_size, 1]\n",
    "        fake = torch.full((realSpectrogram.size(0), 1), fakeLabel, device=DEVICE)  # Ensure it has shape [batch_size, 1]\n",
    "\n",
    "\n",
    "        #Discrim on the real images\n",
    "        dOptimizer.zero_grad() #Zero gradients before training begins\n",
    "        output = discrim(realSpectrogram)\n",
    "        dLossOnReal = lossFunc(output, real)\n",
    "\n",
    "        #Discrim on fake data\n",
    "        noise = torch.randn(realSpectrogram.size(0), len(features)).to(DEVICE)\n",
    "        fakeSpectrogram = generator(noise)\n",
    "        output = discrim(fakeSpectrogram.detach()) #Detach - not training the generator yet\n",
    "        dLossOnFake = lossFunc(output, fake)\n",
    "\n",
    "        #Combined Loss\n",
    "        dLossTotal = dLossOnReal + dLossOnFake\n",
    "        dLossTotal.backward()\n",
    "        dOptimizer.step()\n",
    "        \n",
    "        #Train the generator\n",
    "        gOptimizer.zero_grad() #Zero gradients before training begins\n",
    "        output = discrim(fakeSpectrogram) #not calling deteach - training the generator\n",
    "        output = output.view(-1) #Flatten the output to match label\n",
    "        gLoss = lossFunc(output, real)\n",
    "        gLoss.backward()\n",
    "        gOptimizer.step()\n",
    "\n",
    "        #output so we know something happened\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{GANepochs}], D Loss: {dLossTotal.item():.4f}, G Loss: {gLoss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374d353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
